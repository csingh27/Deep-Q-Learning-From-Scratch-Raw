{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe3209d",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) for Breakout Game\n",
    "\n",
    "This notebook implements the Deep Q-Network (DQN) algorithm for training an agent to play the Breakout game using OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683bf5e2",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e1671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 13:55:18.397611: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 13:55:18.477716: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-31 13:55:18.480028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dfki.uni-bremen.de/csingh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-05-31 13:55:18.480038: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-31 13:55:18.800287: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dfki.uni-bremen.de/csingh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-05-31 13:55:18.800328: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dfki.uni-bremen.de/csingh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-05-31 13:55:18.800331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "disable_eager_execution()\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bae1d",
   "metadata": {},
   "source": [
    "## 2. Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83108b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "STATE_SIZE = (84, 84, 4)\n",
    "ACTION_SIZE = 3\n",
    "\n",
    "# Training parameters\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EXPLORATION_STEPS = 1000000\n",
    "EPSILON_DECAY_STEP = (EPSILON_START - EPSILON_END) / EXPLORATION_STEPS\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_START = 50000\n",
    "UPDATE_TARGET_RATE = 10000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "MEMORY_SIZE = 400000\n",
    "NO_OP_STEPS = 30\n",
    "\n",
    "# Model parameters\n",
    "LEARNING_RATE = 0.00025\n",
    "OPTIMIZER_EPSILON = 0.01\n",
    "\n",
    "# General parameters\n",
    "EPISODES = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796562b",
   "metadata": {},
   "source": [
    "## 3. Define DQNAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0d9d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:43\u001b[0;36m\u001b[0m\n\u001b[0;31m    return np.argmax(q_values[0])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = STATE_SIZE\n",
    "        self.action_size = ACTION_SIZE\n",
    "        \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        \n",
    "        self.gamma = DISCOUNT_FACTOR\n",
    "        \n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        self.epsilon_decay_step = EPSILON_DECAY_STEP\n",
    "        \n",
    "        self.epsilon_end = EPSILON_END\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        \n",
    "        optimizer = RMSprop(lr=LEARNING_RATE, epsilon=OPTIMIZER_EPSILON)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_values = self.model.predict(state)\n",
    "           return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < TRAIN_START:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "        \n",
    "        states = np.zeros((BATCH_SIZE, *self.state_size))\n",
    "        next_states = np.zeros((BATCH_SIZE, *self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            states[i] = minibatch[i][0]\n",
    "            actions.append(minibatch[i][1])\n",
    "            rewards.append(minibatch[i][2])\n",
    "            next_states[i] = minibatch[i][3]\n",
    "            dones.append(minibatch[i][4])\n",
    "        \n",
    "        q_values = self.model.predict(states)\n",
    "        next_q_values = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "        \n",
    "        self.model.fit(states, q_values, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c80ce",
   "metadata": {},
   "source": [
    "## 4. Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85385bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    gray = rgb2gray(frame)\n",
    "    resized = resize(gray, (110, 84), mode='constant')\n",
    "    cropped = resized[18:102, :]\n",
    "    preprocessed = cv2.resize(cropped, (84, 84), interpolation=cv2.INTER_NEAREST)\n",
    "    preprocessed = preprocessed / 255.0\n",
    "    return preprocessed\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([frame, frame, frame, frame], maxlen=4)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "    stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a687f1b",
   "metadata": {},
   "source": [
    "## 5. Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4feee7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DQNAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m                     agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreakout_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dqn\u001b[39m():\n\u001b[1;32m      2\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBreakoutDeterministic-v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m()\n\u001b[1;32m      6\u001b[0m     stacked_frames \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DQNAgent' is not defined"
     ]
    }
   ],
   "source": [
    "def train_dqn():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "    \n",
    "    agent = DQNAgent()\n",
    "    \n",
    "    stacked_frames = deque(maxlen=4)\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        \n",
    "        state = env.reset()\n",
    "        stacked_state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action = agent.get_action(np.expand_dims(stacked_state, axis=0))\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            next_stacked_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            \n",
    "            agent.replay_memory(stacked_state, action, reward, next_stacked_state, done)\n",
    "            \n",
    "            stacked_state = next_stacked_state\n",
    "            \n",
    "            if len(agent.memory) > TRAIN_START:\n",
    "                agent.train_replay()\n",
    "                \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                \n",
    "                print(f'Episode: {episode+1}/{EPISODES}, Score: {total_reward}')\n",
    "                \n",
    "                if episode % 10 == 0:\n",
    "                    agent.model.save_weights('breakout_weights.h5')\n",
    "                \n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c86155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95064ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514aabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
