{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbb02f3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Playing Atari using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd98b0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adca67",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train an agent to play Atari Breakout using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa532f4d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97f02c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The original game frames of shape of (210, 160, 3)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/2/2b/Breakout2600.svg/1920px-Breakout2600.svg.png\" alt=\"Atari Breakout\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e73f5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a0fad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Maximize score by designing a DQN Agent that makes optimal decisions based on the observed game frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e13dd1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Brainstorming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721b9b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Whiteboard](https://www.tutorialspoint.com/whiteboard.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e88ca",
   "metadata": {},
   "source": [
    "# Project Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfd60f",
   "metadata": {},
   "source": [
    "## A1: Import Dependencies\n",
    "\n",
    "This activity involves importing the necessary dependencies for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b257b3",
   "metadata": {},
   "source": [
    "### **A**1.1 Import RL framework, OpenCV and relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6d9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TAKS Import gymnasium, numpy, random, skimage, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae59b8",
   "metadata": {},
   "source": [
    "### **A**1.2 Import tensorflow modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16ff3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TAKS Import tensorflow models, layers, optimizers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e4153",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0a3c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Which tensorflow modules are essential for any deep learning project?\n",
    "2. How do you import convolution layers module in tensorflow?\n",
    "3. What is sequential in TensorFlow keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402285fc",
   "metadata": {},
   "source": [
    "## A2: Define gym environment\n",
    "\n",
    "This activity involves defining the constants for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abdfe0",
   "metadata": {},
   "source": [
    "### **A**2.1 Define RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df5d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0044cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Create gym loop and make gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bafab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize done, dead, step, score, start_life\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        # Initialize the variables\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2341bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EPISODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mEPISODES\u001b[49m):\n\u001b[1;32m      5\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         dead \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPISODES' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            # Take a random action\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063851d",
   "metadata": {},
   "source": [
    "### **A**2.2 Take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13154a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      8\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Define start_life and dead variables\n",
    "# if the agent missed the ball, agent is dead but episode is not over\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            # TASK Define start_life and dead variables\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Clip reward between -1 and 1\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            # Clip reward between -1 and 1\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e01a01",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9f101",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. How do you take a random action in a gymnasium environment?\n",
    "2. What are the conditions under which the episode ends for the Atari Breakout scenario?\n",
    "3. Define how to display the frames in Atari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f9419",
   "metadata": {},
   "source": [
    "## A3: Preprocessing Functions\n",
    "\n",
    "Thisactivity involves defining the preprocessing functions used to transform the game frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef67d70",
   "metadata": {},
   "source": [
    "### **A**3.1 Define pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51f3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Define a function to pre-process the image frames from (210, 160, 3) to (84, 84, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cba094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Update the gym environment to include pre-processed image\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        # TASK: Add pre-processed state here\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            # TASK: Add pre-processed state here \n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf551d",
   "metadata": {},
   "source": [
    "### **A**3.2 Define history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize initial history\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        # TASK: Create a stack of 4 states to define the history\n",
    "        # TASK: Re-shape the history to (1, 84, 84, 4)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize next history\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            # Re-shape next_state to (1 , 84, 84, 1)\n",
    "            # Define next history\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8550f8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cdcf3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. What is the purpose of the `preprocess_frame()` function?\n",
    "2. Explain the use of the `stacked_frames` parameter in the `stack_frames()` function.\n",
    "3. How is the state defined in the Atari Breakout environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f87f53",
   "metadata": {},
   "source": [
    "## A4: Create DQNAgent Class\n",
    "\n",
    "This activity involves defining the `DQNAgent` class, which implements the Deep Q-Network (DQN) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d57f68",
   "metadata": {},
   "source": [
    "### **A**4.1 Initialize DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# Define a class DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d894a05",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3882795768.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [25]\u001b[0;36m\u001b[0m\n\u001b[0;31m    # TASK - Initialize render and load_model variables\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        # TASK - Initialize render and load_model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616c661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK - Define environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "179eaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # TASK: Define exploration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7628530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        # TASK- Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b714e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        # TASK- Initialize optimiyer, sess, av_q_max, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b34397",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **A**4.2 Initialize functions for DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45665ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK - Define optimizer function\n",
    "    # if the error is in [-1, 1], then the cost is quadratic to the error\n",
    "    # But outside the interval, the cost is linear to the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf6939b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # approximate Q function using Convolution Neural Network\n",
    "    # state is input and Q Value of each action is output of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc74a53b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # after some time interval update the target model to be same with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252bd7f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "   # get action from model using epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e18dc9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # save sample <s,a,r,s'> to the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2324d830",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # pick samples randomly from replay memory (with batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f71ba45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK - Define save_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f9f0a4a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # make summary operators for tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bd690",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e2927",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. What is the purpose of the `build_model()` method in the `DQNAgent` class?\n",
    "2. Explain the purpose of the `Conv2D` layers in the neural network.\n",
    "3. What is the activation function used in the last `Dense` layer of the model, and why is it chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de80b18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A5: Main Training Loop\n",
    "\n",
    "This activity involves defining the main training loop for the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9b351",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**5.1 Predict action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d4a2133",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPISODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mEPISODES\u001b[49m):\n\u001b[1;32m      5\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         observe \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPISODES' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        # TASK- Take empty steps in the beginning\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea0a64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            # Update steps\n",
    "            # TASK- get action for the current history and go one step in environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832e73a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            # TASK- Change the action from random action to DQN policy\n",
    "            # action = \n",
    "            # action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf88d0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**5.2 Perform Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fdfd97",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    # TASK- Define DQN agent\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776cb7b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**5.3 - Sample from replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94851c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    agent = DQNAgent(action_size=3)\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # TASK- Save the sample <a, a, r, s'> to the replay memory\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40095c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    agent = DQNAgent(action_size=3)\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            # TASK- If agent is dead, then reset the history\n",
    "\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2764c23",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    agent = DQNAgent(action_size=3)\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward\n",
    "            # TASK - Plot the score over episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad835b6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1c115",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. How does the DQN agent update its Q-network during training?\n",
    "2. What is the purpose of the train_replay method in the DQN loop, and when is it called?\n",
    "3. Explain the role of the loss variable in the DQN loop and how it is calculated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffec46",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7729e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://github.com/rlcode/reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "440.708px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
