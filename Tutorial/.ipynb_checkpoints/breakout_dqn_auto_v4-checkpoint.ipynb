{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbb02f3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Playing Atari using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd98b0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adca67",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train an agent to play Atari Breakout using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa532f4d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97f02c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The original game frames of shape of (210, 160, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e73f5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a0fad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Maximize score by making optimal decisions based on the observed game frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e13dd1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Brainstorming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721b9b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Whiteboard](https://www.tutorialspoint.com/whiteboard.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfd60f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A1: Import Dependencies\n",
    "\n",
    "This activity involves importing the necessary dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedaef66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TAKS Import gymnasium, tensorflow and any necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae986ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 11:39:21.724117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 11:39:21.804431: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-01 11:39:21.806738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dfki.uni-bremen.de/csingh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-01 11:39:21.806747: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-01 11:39:22.209066: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dfki.uni-bremen.de/csingh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-01 11:39:22.209116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu:/home/dfki.uni-bremen.de/csingh/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-06-01 11:39:22.209120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.object, string),\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.bool, bool),\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_STRING: np.object,\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:597: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_BOOL: np.bool,\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:614: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_STRING_REF: np.object,\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:619: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  types_pb2.DT_BOOL_REF: np.bool,\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "WARNING:tensorflow:From /home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import cv2\n",
    "tf.disable_v2_behavior()\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e4153",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assessment\n",
    "\n",
    "1. What is the purpose of importing dependencies in a Python project?\n",
    "2. Why do we use the `gym` library?\n",
    "3. Explain the role of the `Conv2D` layer in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402285fc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A2: Define gym environment\n",
    "\n",
    "This activity involves defining the constants for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df5d54b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3b29d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32172eeb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Create gym loop and make gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea779c0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      6\u001b[0m         observe \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98884c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize done, dead, step, score, start_life\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        # Initialize the variables\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492213f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081e61f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            # Take a random action\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22686a5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe4758",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Define start_life and dead variables\n",
    "# if the agent missed the ball, agent is dead but episode is not over\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            # TASK Define start_life and dead variables\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590459c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7eb2c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Clip reward between -1 and 1\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            # Clip reward between -1 and 1\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc214cb",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e01a01",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assessment\n",
    "\n",
    "1. What is the purpose of defining constants in a Python project?\n",
    "2. Explain the significance of the `GAMMA` constant in the context of reinforcement learning.\n",
    "3. How does the `EPSILON_DECAY` constant affect the agent's exploration-exploitation tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f9419",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A2: Preprocessing Functions\n",
    "\n",
    "Thisactivity involves defining the preprocessing functions used to transform the game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f3063",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Define a function to pre-process the image frames from (210, 160, 3) to (84, 84, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62aa7db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "def pre_processing(observe):\n",
    "\n",
    "    # A2.1 Convert to gray scale \n",
    "    # A2.2 Resize\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cba094",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Update the gym environment to include pre-processed image\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        # TASK: Add pre-processed state here\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            # TASK: Add pre-processed state here \n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84892c",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d214f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize initial history\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        # TASK: Create a stack of 4 states to define the history\n",
    "        # TASK: Re-shape the history to (1, 84, 84, 4)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b780e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6c84e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize next history\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            # Re-shape next_state to (1 , 84, 84, 1)\n",
    "            # Define next history\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27480331",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8550f8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assessment\n",
    "\n",
    "1. What is the purpose of the `preprocess_frame()` function?\n",
    "2. Explain the use of the `stacked_frames` parameter in the `stack_frames()` function.\n",
    "3. How does the `is_new_episode` parameter affect the stacking of frames?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f87f53",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A3: Create DQNAgent Class\n",
    "\n",
    "This activity involves defining the `DQNAgent` class, which implements the Deep Q-Network (DQN) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf4249",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# Define a class DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1bbf0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5eea77",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        # TASK - Initialize render and load_model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a52185",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7807e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK - Define environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691f86b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb552d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # TASK: Define exploration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b14b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01190e35",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad058be9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac80684f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        # TASK- Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b10ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d78b66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        # TASK- Initialize optimiyer, sess, av_q_max, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff92aff2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        # TASK- Initialize optimiyer, sess, av_q_max, etc.\n",
    "        self.optimizer = self.optimizer()\n",
    "        self.sess = tf.compat.v1.InteractiveSession()\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = \\\n",
    "            self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            'summary/breakout_dqn', self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/breakout_dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a517a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK - Define optimizer function\n",
    "    # if the error is in [-1, 1], then the cost is quadratic to the error\n",
    "    # But outside the interval, the cost is linear to the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8e3ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(learning_rate=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c32de",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # approximate Q function using Convolution Neural Network\n",
    "    # state is input and Q Value of each action is output of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001b636",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                         input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56607d34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # after some time interval update the target model to be same with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d86bab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "   def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe97464",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "   # get action from model using epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433d120",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "   def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2aa450",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # save sample <s,a,r,s'> to the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037f744",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def replay_memory(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d251e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # pick samples randomly from replay memory (with batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7b67f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                                 self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        # like Q Learning, get maximum Q value at s'\n",
    "        # But from target model\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1beffb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK - Define save_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4424aff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "     def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc78eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # make summary operators for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c95e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaef326",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A3.1: Build Model\n",
    "\n",
    "This sub-activity involves defining the `build_model()` method inside the `DQNAgent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb813022",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(84, 84, 4)))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(4, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bd690",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment\n",
    "\n",
    "1. What is the purpose of the `build_model()` method in the `DQNAgent` class?\n",
    "2. Explain the purpose of the `Conv2D` layers in the neural network.\n",
    "3. What is the activation function used in the last `Dense` layer of the model, and why is it chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de80b18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A5: Main Training Loop\n",
    "\n",
    "This activity involves defining the main training loop for the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4a2133",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/skimage/color/colorconv.py:135: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the input array must have size 3 along `channel_axis`, got (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      6\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 7\u001b[0m     stacked_frames, state \u001b[38;5;241m=\u001b[39m \u001b[43mstack_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mstack_frames\u001b[0;34m(stacked_frames, state, is_new_episode)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack_frames\u001b[39m(stacked_frames, state, is_new_episode):\n\u001b[0;32m----> 9\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_new_episode:\n\u001b[1;32m     11\u001b[0m         stacked_frames \u001b[38;5;241m=\u001b[39m deque([frame] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mpreprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_frame\u001b[39m(frame):\n\u001b[0;32m----> 2\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mrgb2gray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     frame \u001b[38;5;241m=\u001b[39m resize(frame, (\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     frame \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/skimage/_shared/utils.py:394\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/skimage/color/colorconv.py:875\u001b[0m, in \u001b[0;36mrgb2gray\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;129m@channel_as_last_axis\u001b[39m(multichannel_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrgb2gray\u001b[39m(rgb, \u001b[38;5;241m*\u001b[39m, channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute luminance of an RGB image.\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m    >>> img_gray = rgb2gray(img)\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_colorarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m     coeffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.2125\u001b[39m, \u001b[38;5;241m0.7154\u001b[39m, \u001b[38;5;241m0.0721\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mrgb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rgb \u001b[38;5;241m@\u001b[39m coeffs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/skimage/color/colorconv.py:140\u001b[0m, in \u001b[0;36m_prepare_colorarray\u001b[0;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape[channel_axis] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe input array must have size 3 along `channel_axis`, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    139\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    142\u001b[0m float_dtype \u001b[38;5;241m=\u001b[39m _supported_float_type(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m float_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n",
      "\u001b[0;31mValueError\u001b[0m: the input array must have size 3 along `channel_axis`, got (2,)"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        # TASK- Take empty steps in the beginning\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc71e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f34d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            # Update steps\n",
    "            # TASK- get action for the current history and go one step in environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38186c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710629d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            # TASK- Change the action from random action to DQN policy\n",
    "            # action = \n",
    "            # action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d2cf3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            # TASK- Change the action from random action to DQN policy\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd38bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d18649",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f8b10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # TASK- Save the sample <a, a, r, s'> to the replay memory\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a72d9b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # TASK- Save the sample <a, a, r, s'> to the replay memory\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9e6f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            # TASK- If agent is dead, then reset the history\n",
    "\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6b3f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            # TASK- If agent is dead, then reset the history\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979feee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward\n",
    "            # TASK - Plot the score over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433a7cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward\n",
    "\n",
    "            # if done, plot the score over episodes\n",
    "            if done:\n",
    "                if global_step > agent.train_start:\n",
    "                    stats = [score, agent.avg_q_max / float(step), step,\n",
    "                             agent.avg_loss / float(step)]\n",
    "                    for i in range(len(stats)):\n",
    "                        agent.sess.run(agent.update_ops[i], feed_dict={\n",
    "                            agent.summary_placeholders[i]: float(stats[i])\n",
    "                        })\n",
    "                    summary_str = agent.sess.run(agent.summary_op)\n",
    "                    agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon,\n",
    "                      \"  global_step:\", global_step, \"  average_q:\",\n",
    "                      agent.avg_q_max / float(step), \"  average loss:\",\n",
    "                      agent.avg_loss / float(step))\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/breakout_dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad835b6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assessment\n",
    "\n",
    "1. What is the purpose of the `act()` method in the `DQNAgent` class?\n",
    "2. Explain the role of the `update_epsilon()` method in the training loop.\n",
    "3. Why do we update the target model using the `update_target_model()` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49747ec2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fa948",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
