{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity A1: Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A1.1: List the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A1.2: Explain the dependencies\n",
    "\n",
    "The required dependencies for this activity are as follows:\n",
    "\n",
    "- `gym`: OpenAI Gym library for creating game environments\n",
    "- `numpy`: Library for numerical computing with arrays\n",
    "- `cv2`: OpenCV library for image processing\n",
    "- `collections.deque`: Double-ended queue for efficient memory storage\n",
    "- `skimage.color.rgb2gray`: Function to convert RGB images to grayscale\n",
    "- `skimage.transform.resize`: Function to resize images\n",
    "- `tensorflow.keras.models.Sequential`: Class for creating the deep learning model\n",
    "- `tensorflow.keras.layers`: Classes for defining the layers of the model\n",
    "- `tensorflow.keras.optimizers.Adam`: Optimizer for training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity A2: Define Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A2.1: Define the constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 100\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_START = 1000\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "GAMMA = 0.99\n",
    "LR = 0.00025\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.9995\n",
    "FRAME_SKIP = 4\n",
    "STACK_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A2.2: Explain the constant values\n",
    "\n",
    "The constant values used in this activity are as follows:\n",
    "\n",
    "- `EPISODES`: The number of episodes to train the agent\n",
    "- `BATCH_SIZE`: The batch size for training the DQN model\n",
    "- `TRAIN_START`: The number of steps before starting training\n",
    "- `TARGET_UPDATE_FREQ`: The frequency at which to update the target model\n",
    "- `GAMMA`: The discount factor for future rewards\n",
    "- `LR`: The learning rate for the Adam optimizer\n",
    "- `EPSILON`: The exploration rate for the epsilon-greedy policy\n",
    "- `EPSILON_MIN`: The minimum exploration rate\n",
    "- `EPSILON_DECAY`: The rate at which to decay the exploration rate\n",
    "- `FRAME_SKIP`: The number of frames to skip for each action\n",
    "- `STACK_SIZE`: The number of frames to stack as input to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity A3: Create DQNAgent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A3.1: Define the `DQNAgent` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, num_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.epsilon = EPSILON\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n"
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(8, 8), strides=(4, 4), activation='relu', input_shape=self.state_shape))\n",
    "        model.add(Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "        model.compile(optimizer=Adam(learning_rate=LR), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_replay(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                q_future = max(self.target_model.predict(next_state)[0])\n",
    "                target[0][action] = reward + q_future * GAMMA\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon *= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A3.2: Explain the `DQNAgent` class\n",
    "\n",
    "The `DQNAgent` class is responsible for defining and managing the DQN agent.\n",
    "\n",
    "- `__init__`: Initializes the agent with the state shape and number of actions\n",
    "- `build_model`: Builds the DQN model using a Sequential model with convolutional and dense layers\n",
    "- `update_target_model`: Updates the target model by copying the weights from the main model\n",
    "- `act`: Selects an action based on the epsilon-greedy policy\n",
    "- `replay_memory`: Adds the state, action, reward, next state, and done flag to the agent's memory\n",
    "- `train_replay`: Performs training using a random sample from the replay memory and updates the epsilon value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity A4: Define Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A4.1: Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent):\n",
    "    scores = []\n",
    "    epsilons = []\n",
    "    for episode in range(EPISODES):\n",
    "        state = preprocess_state(env.reset())\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            agent.replay_memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if len(agent.memory) > TRAIN_START:\n",
    "                agent.train_replay()\n",
    "        scores.append(score)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        if episode % TARGET_UPDATE_FREQ == 0:\n",
    "            agent.update_target_model()\n",
    "        print('Episode: {}/{}, Score: {}, Epsilon: {:.2f}'.format(episode, EPISODES, score, agent.epsilon))\n",
    "    return scores, epsilons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A4.2: Explain the training loop\n",
    "\n",
    "The training loop is responsible for training the DQN agent by interacting with the environment.\n",
    "\n",
    "- `train_agent`: Performs the training loop for the specified number of episodes\n",
    "- `preprocess_state`: Preprocesses the state by converting it to grayscale and resizing\n",
    "- `act`: Selects an action using the agent's epsilon-greedy policy\n",
    "- `replay_memory`: Adds the state, action, reward, next state, and done flag to the agent's memory\n",
    "- `train_replay`: Performs training using a random sample from the replay memory\n",
    "- `update_target_model`: Updates the target model by copying the weights from the main model\n",
    "- Prints the episode number, score, and epsilon value for each episode\n",
    "- Returns the scores and epsilons for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity A5: Test and Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A5.1: Test the trained agent on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent):\n",
    "    state = preprocess_state(env.reset())\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = preprocess_state(next_state)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        env.render()\n",
    "    print('Final Score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-activity A5.2: Analyze the performance of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(scores, epsilons):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.plot(scores)\n",
    "    plt.title('Scores over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(epsilons)\n",
    "    plt.title('Epsilon over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    "1. What is the purpose of the `DQNAgent` class?\n",
    "2. What are the main components of the DQN model?\n",
    "3. Explain the epsilon-greedy policy used for action selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
