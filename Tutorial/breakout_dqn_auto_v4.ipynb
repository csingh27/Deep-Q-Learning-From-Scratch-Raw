{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbb02f3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Playing Atari using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd98b0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adca67",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train an agent to play Atari Breakout using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa532f4d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97f02c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The original game frames of shape of (210, 160, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e73f5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a0fad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Maximize score by designing a DQN Agent that makes optimal decisions based on the observed game frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e13dd1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Brainstorming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721b9b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Whiteboard](https://www.tutorialspoint.com/whiteboard.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e88ca",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Project Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfd60f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A1: Import Dependencies\n",
    "\n",
    "This activity involves importing the necessary dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedaef66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TAKS Import gymnasium, numpy, random, skimage, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b257b3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**1.1 Import RL framework, OpenCV and relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f645f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae59b8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**1.2 Import tensorflow modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ff3c6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TAKS Import tensorflow models, layers, optimizers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae986ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "tf.disable_v2_behavior()\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e4153",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0a3c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Which tensorflow modules are essential for any deep learning project?\n",
    "2. How do you import convolution layers module in tensorflow?\n",
    "3. What is sequential in TensorFlow keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402285fc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A2: Define gym environment\n",
    "\n",
    "This activity involves defining the constants for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abdfe0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**2.1 Define RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df5d54b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b3b29d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0044cd56",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Create gym loop and make gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2444b2f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      5\u001b[0m     observe \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdone\u001b[49m:\n\u001b[1;32m      7\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bafab3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize done, dead, step, score, start_life\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        # Initialize the variables\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7b1c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2341bf9b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TASK BLOCK\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      5\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            # Take a random action\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063851d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**2.2 Take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067b9196",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      6\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13154a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      8\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Define start_life and dead variables\n",
    "# if the agent missed the ball, agent is dead but episode is not over\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            # TASK Define start_life and dead variables\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2359a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      6\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4f085",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Clip reward between -1 and 1\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            # Clip reward between -1 and 1\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde00583",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e01a01",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9f101",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. How do you take a random action in a gymnasium environment?\n",
    "2. What are the conditions under which the episode ends for the Atari Breakout scenario?\n",
    "3. Define how to display the frames in Atari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f9419",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A3: Preprocessing Functions\n",
    "\n",
    "Thisactivity involves defining the preprocessing functions used to transform the game frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef67d70",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**3.1 Define pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51f3063",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Define a function to pre-process the image frames from (210, 160, 3) to (84, 84, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62aa7db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "def pre_processing(observe):\n",
    "\n",
    "    # A2.1 Convert to gray scale \n",
    "    # A2.2 Resize\n",
    "    observe = np.asarray(observe[0])\n",
    "    print(observe.shape)\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cba094",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Update the gym environment to include pre-processed image\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        # TASK: Add pre-processed state here\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            # TASK: Add pre-processed state here \n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a84892c",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 13\u001b[0m     observe, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m pre_processing(observe)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_life \u001b[38;5;241m>\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/shimmy/atari_env.py:294\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    292\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 294\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf551d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**3.2 Define history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340d55e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize initial history\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        # TASK: Create a stack of 4 states to define the history\n",
    "        # TASK: Re-shape the history to (1, 84, 84, 4)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b1f5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c810f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK Initialize next history\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            # Re-shape next_state to (1 , 84, 84, 1)\n",
    "            # Define next history\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33aa07",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n",
      "(160, 3)\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        dead = False\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8550f8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cdcf3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. What is the purpose of the `preprocess_frame()` function?\n",
    "2. Explain the use of the `stacked_frames` parameter in the `stack_frames()` function.\n",
    "3. How is the state defined in the Atari Breakout environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f87f53",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A4: Create DQNAgent Class\n",
    "\n",
    "This activity involves defining the `DQNAgent` class, which implements the Deep Q-Network (DQN) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d57f68",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**4.1 Initialize DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f3c39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# Define a class DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cd05d68",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2928508518.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [24]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, action_size):\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d894a05",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3882795768.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [25]\u001b[0;36m\u001b[0m\n\u001b[0;31m    # TASK - Initialize render and load_model variables\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        # TASK - Initialize render and load_model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c422d70c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616c661d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK - Define environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be2ed86",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "179eaf25",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # TASK: Define exploration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfe94651",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2275f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f123292",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7628530",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        # TASK- Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037377db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b714e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        # TASK- Initialize optimiyer, sess, av_q_max, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff92aff2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        # TASK: Define environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        # TASK- Define training parameters        \n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        # TASK- Initialize optimiyer, sess, av_q_max, etc.\n",
    "        self.optimizer = self.optimizer()\n",
    "        self.sess = tf.compat.v1.InteractiveSession()\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = \\\n",
    "            self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            'summary/breakout_dqn', self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/breakout_dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b34397",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**4.2 Initialize functions for DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e45665ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK - Define optimizer function\n",
    "    # if the error is in [-1, 1], then the cost is quadratic to the error\n",
    "    # But outside the interval, the cost is linear to the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3051e582",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32') # Input values for action indices\n",
    "        y = K.placeholder(shape=(None,), dtype='float32') # Target Q-Value\n",
    "\n",
    "        py_x = self.model.output # Predicted Q-Value\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size) # One-hot encoding of action indices\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1) # Q-Value for the selected action\n",
    "        error = K.abs(y - q_value) # Absolute difference between the Target Q-Value and the predicted Q-Value\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)# Error clipped to limit the impact of large values on the training process\n",
    "        linear_part = error - quadratic_part # Linear part of the error\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(learning_rate=0.00025, epsilon=0.01) # The RMSprop optimizer is used to update the weights of the neural network during training\n",
    "        updates = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebf6939b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # approximate Q function using Convolution Neural Network\n",
    "    # state is input and Q Value of each action is output of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c08102e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                         input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc74a53b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # after some time interval update the target model to be same with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "977b8b64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "   def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "252bd7f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "   # get action from model using epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5c7b17",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "   def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95e18dc9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # save sample <s,a,r,s'> to the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e58d9fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def replay_memory(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2324d830",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # pick samples randomly from replay memory (with batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dec98a72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                                 self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        # like Q Learning, get maximum Q value at s'\n",
    "        # But from target model\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f71ba45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "# TASK - Define save_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d7b5864",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "class DQNAgent:\n",
    "     def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f9f0a4a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "    # make summary operators for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71982da3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bd690",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e2927",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. What is the purpose of the `build_model()` method in the `DQNAgent` class?\n",
    "2. Explain the purpose of the `Conv2D` layers in the neural network.\n",
    "3. What is the activation function used in the last `Dense` layer of the model, and why is it chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de80b18",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A5: Main Training Loop\n",
    "\n",
    "This activity involves defining the main training loop for the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9b351",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**5.1 Predict action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d4a2133",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPISODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mEPISODES\u001b[49m):\n\u001b[1;32m      5\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         observe \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPISODES' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        # TASK- Take empty steps in the beginning\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20dab4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea0a64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            env.render()\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            # Update steps\n",
    "            # TASK- get action for the current history and go one step in environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e312d60",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832e73a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            # TASK- Change the action from random action to DQN policy\n",
    "            # action = \n",
    "            # action = env.action_space.sample()\n",
    "            observe, reward, done, truncate, info = env.step(action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591a5e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            # TASK- Change the action from random action to DQN policy\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf88d0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**5.2 Perform Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fdfd97",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8ccbf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776cb7b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **A**5.3 - Sample from replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94851c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "EPISODES = 50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # TASK- Save the sample <a, a, r, s'> to the replay memory\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df7628",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # TASK- Save the sample <a, a, r, s'> to the replay memory\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40095c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            # TASK- If agent is dead, then reset the history\n",
    "\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8cae0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            # TASK- If agent is dead, then reset the history\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2764c23",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TASK BLOCK\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward\n",
    "            # TASK - Plot the score over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad23e0c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fatal Python error: pygame_parachute: (pygame parachute) Segmentation Fault\n",
      "Python runtime state: initialized\n",
      "\n",
      "Thread 0x00007fe2c37fe700 (most recent call first):\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/ipykernel/parentpoller.py\", line 36 in run\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/threading.py\", line 973 in _bootstrap_inner\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/threading.py\", line 930 in _bootstrap\n",
      "\n",
      "Thread 0x00007fe2c3fff700 (most recent call first):\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/IPython/core/history.py\", line 762 in _writeout_input_cache\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/IPython/core/history.py\", line 779 in writeout_cache\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/lib/python3.9/site-packages/IPython/core/history.py\", line 60 in only_when_enabled\n",
      "  File \"/home/dfki.uni-bremen.de/csingh/anaconda3/l"
     ]
    }
   ],
   "source": [
    "# SOLUTION BLOCK\n",
    "EPISODES=50000\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Breakout-v5\", render_mode = 'human')\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        observe = env.reset()\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "        while not done:\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "            observe, reward, done, truncate, info = env.step(real_action)\n",
    "            # TASK- Define average Q-max\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "            score += reward\n",
    "\n",
    "            # if done, plot the score over episodes\n",
    "            if done:\n",
    "                if global_step > agent.train_start:\n",
    "                    stats = [score, agent.avg_q_max / float(step), step,\n",
    "                             agent.avg_loss / float(step)]\n",
    "                    for i in range(len(stats)):\n",
    "                        agent.sess.run(agent.update_ops[i], feed_dict={\n",
    "                            agent.summary_placeholders[i]: float(stats[i])\n",
    "                        })\n",
    "                    summary_str = agent.sess.run(agent.summary_op)\n",
    "                    agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon,\n",
    "                      \"  global_step:\", global_step, \"  average_q:\",\n",
    "                      agent.avg_q_max / float(step), \"  average loss:\",\n",
    "                      agent.avg_loss / float(step))\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/breakout_dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad835b6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1c115",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. How does the DQN agent update its Q-network during training?\n",
    "2. What is the purpose of the train_replay method in the DQN loop, and when is it called?\n",
    "3. Explain the role of the loss variable in the DQN loop and how it is calculated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffec46",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7729e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* https://github.com/rlcode/reinforcement-learning"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "440.708px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
