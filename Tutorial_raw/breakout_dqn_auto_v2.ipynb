{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Concepts\n",
    "\n",
    "Before we begin, let's understand some theoretical concepts:\n",
    "\n",
    "1. Reinforcement Learning: Reinforcement Learning is a type of machine learning where an agent learns to make decisions in an environment to maximize a reward signal.\n",
    "2. Q-Learning: Q-Learning is a model-free, off-policy algorithm used in reinforcement learning. It aims to learn the action-value function, known as Q-function, which maps states and actions to their expected rewards.\n",
    "3. Deep Q-Network (DQN): Deep Q-Network is a variant of Q-Learning that uses a deep neural network as a function approximator to estimate the Q-values for state-action pairs.\n",
    "\n",
    "Now, let's dive into the activities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity A1: Environment Setup\n",
    "\n",
    "### Sub-activity A1.1: Installing Required Packages\n",
    "\n",
    "In this sub-activity, we'll install the necessary packages for our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install gym\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. Why is it important to install the required packages for our environment?\n",
    "2. What is the purpose of installing the `gym` package?\n",
    "3. Which package is used for working with images in this activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-activity A1.2: Importing Required Libraries\n",
    "\n",
    "In this sub-activity, we'll import the necessary libraries for our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import cv2\n",
    "\n",
    "# Disable eager execution for compatibility\n",
    "tf.disable_v2_behavior()\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. What is the purpose of importing the `gym` library?\n",
    "2. Why do we disable eager execution in TensorFlow?\n",
    "3. Which library is used for image processing in this activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity A2: Preprocessing the Environment\n",
    "\n",
    "### Sub-activity A2.1: Creating the Preprocessing Functions\n",
    "\n",
    "In this sub-activity, we'll define the preprocessing functions to prepare the environment observations for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = rgb2gray(frame)\n",
    "    frame = resize(frame, (84, 84), mode='constant')\n",
    "    frame *= 255\n",
    "    frame = np.uint8(frame)\n",
    "    return frame\n",
    "\n",
    "def stack_frames(stacked_frames, frame, is_new_episode):\n",
    "    frame = preprocess_frame(frame)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for _ in range(4)], maxlen=4)\n",
    "        for _ in range(4):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "    stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    return stacked_frames, stacked_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. What is the purpose of the `preprocess_frame` function?\n",
    "2. How does the `stack_frames` function handle new episodes?\n",
    "3. What is the shape of the stacked state returned by the `stack_frames` function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-activity A2.2: Preprocessing the Environment Observations\n",
    "\n",
    "In this sub-activity, we'll preprocess the environment observations before feeding them to the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Preprocess the initial observation\n",
    "preprocessed_observation = preprocess_frame(initial_observation)\n",
    "\n",
    "# Initialize the deque that will store the stacked frames\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for _ in range(4)], maxlen=4)\n",
    "\n",
    "# Preprocess the initial observation and stack the frames\n",
    "for _ in range(4):\n",
    "    stacked_frames.append(preprocessed_observation)\n",
    "\n",
    "# Create the stacked state\n",
    "stacked_state = np.stack(stacked_frames, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. What is the purpose of preprocessing the environment observations?\n",
    "2. How do we stack the preprocessed frames in the `stacked_frames` deque?\n",
    "3. What is the shape of the stacked state after preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity A3: Building the Deep Q-Network\n",
    "\n",
    "### Sub-activity A3.1: Creating the Q-Network Architecture\n",
    "\n",
    "In this sub-activity, we'll define the architecture of the Deep Q-Network (DQN) using a convolutional neural network (CNN).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_q_network(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=state_size))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation=None))\n",
    "    return model\n",
    "\n",
    "# Create the Q-network\n",
    "q_network = create_q_network((84, 84, 4), env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. What is the purpose of the `create_q_network` function?\n",
    "2. How many convolutional layers are there in the Q-network architecture?\n",
    "3. What is the activation function used in the last dense layer of the Q-network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-activity A3.2: Initializing the Q-Network\n",
    "\n",
    "In this sub-activity, we'll initialize the Q-network and define the hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the hyperparameters\n",
    "state_size = (84, 84, 4)\n",
    "action_size = env.action_space.n\n",
    "learning_rate = 0.00025\n",
    "gamma = 0.99\n",
    "epsilon_initial = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 1e-6\n",
    "replay_memory_size = 1000000\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the Q-network\n",
    "q_network = create_q_network(state_size, action_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = tf.losses.Huber()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "# Define the replay memory\n",
    "replay_memory = deque(maxlen=replay_memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. What is the purpose of defining hyperparameters for the Q-network?\n",
    "2. What is the role of the loss function in the training process?\n",
    "3. What is the purpose of the replay memory in reinforcement learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity A4: Training the Deep Q-Network\n",
    "\n",
    "### Sub-activity A4.1: Implementing the Training Loop\n",
    "\n",
    "In this sub-activity, we'll implement the training loop to train the Deep Q-Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the training loop\n",
    "def train_q_network(num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        stacked_frames, stacked_state = stack_frames(stacked_frames, state, True)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            # Select an action using epsilon-greedy exploration\n",
    "            epsilon = epsilon_final + (epsilon_initial - epsilon_final) * np.exp(-epsilon_decay * step)\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = q_network.predict(np.expand_dims(stacked_state, axis=0))\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "            # Take the action and observe the next state, reward, and done flag\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            stacked_frames, next_stacked_state = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "            # Store the transition in the replay memory\n",
    "            replay_memory.append((stacked_state, action, reward, next_stacked_state, done))\n",
    "\n",
    "            # Update the current state\n",
    "            stacked_state = next_stacked_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Sample a random batch from the replay memory\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            # Convert the batch to arrays\n",
    "            states = np.array(states)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            next_states = np.array(next_states)\n",
    "            dones = np.array(dones)\n",
    "\n",
    "            # Compute the target Q-values\n",
    "            target_q_values = q_network.predict(next_states)\n",
    "            target_q_values[dones] = np.zeros((action_size,))\n",
    "            target_q_values = rewards + gamma * np.max(target_q_values, axis=1)\n",
    "\n",
    "            # Train the Q-network\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = q_network(states)\n",
    "                q_values = tf.reduce_sum(tf.one_hot(actions, action_size) * q_values, axis=1)\n",
    "                loss = loss_function(target_q_values, q_values)\n",
    "            gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "        # Print the episode information\n",
    "        print(f'Episode: {episode + 1} | Total Reward: {total_reward}')\n",
    "\n",
    "# Train the Q-network for 1000 episodes\n",
    "train_q_network(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment\n",
    "\n",
    "1. How does the epsilon-greedy exploration strategy work?\n",
    "2. What is the purpose of the replay memory in the training loop?\n",
    "3. What is the role of the loss function and optimizer in training the Q-network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully completed the activities for building and training a Deep Q-Network (DQN) for reinforcement learning.\n",
    "\n",
    "These activities covered important concepts such as reinforcement learning, Q-Learning, and Deep Q-Networks. You learned how to set up the environment, preprocess the observations, build the Q-network architecture, and implement the training loop.\n",
    "\n",
    "Keep exploring and experimenting with reinforcement learning algorithms and techniques to further enhance your understanding and skills!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
