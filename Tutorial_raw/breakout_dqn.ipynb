{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import cv2\n",
    "tf.disable_v2_behavior()\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b48374",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimizer = self.optimizer()\n",
    "\n",
    "        self.sess = tf.compat.v1.InteractiveSession()\n",
    "        # K.set_session(self.sess)\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = \\\n",
    "            self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            'summary/breakout_dqn', self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/breakout_dqn.h5\")\n",
    "\n",
    "    # if the error is in [-1, 1], then the cost is quadratic to the error\n",
    "    # But outside the interval, the cost is linear to the error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(learning_rate=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ef900",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # approximate Q function using Convolution Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                         input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6af5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48283dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                                 self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        # like Q Learning, get maximum Q value at s'\n",
    "        # But from target model\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79482a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    # make summary operators for tensorboard\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def pre_processing(observe):\n",
    "\n",
    "    # A2.1 Convert to gray scale \n",
    "    # A2.2 Resize\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # In case of BreakoutDeterministic-v3, always skip 4 frames\n",
    "    # Deterministic-v4 version use 4 actions\n",
    "\n",
    "    # A1 Environment\n",
    "    # A1.1 Install gym\n",
    "    # A1.2 Create gym environment\n",
    "\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "    agent = DQNAgent(action_size=3)\n",
    "\n",
    "    scores, episodes, global_step = [], [], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for e in range(EPISODES):\n",
    "\n",
    "        # A1.3 Initialize variables\n",
    "        done = False\n",
    "        dead = False\n",
    "        # 1 episode = 5 lives\n",
    "        step, score, start_life = 0, 0, 5\n",
    "\n",
    "        # A1.4 Reset environment \n",
    "        observe = env.reset()\n",
    "\n",
    "        # A1.5 Take empty steps in the beginning\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _,_ = env.step(1)\n",
    "\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "\n",
    "        # A2 Dataset pre-processing\n",
    "        state = pre_processing(observe)\n",
    "\n",
    "        # A2.2 Create initial history\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        # A2.3 Re-shape history\n",
    "        history = np.reshape([history], (1, 84, 84, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # A3 Fill replay buffer\n",
    "        # A3.1 Define loop and variables\n",
    "        while not done:\n",
    "            # A3.2 Render environment\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # get action for the current history and go one step in environment\n",
    "            # A3.2 Ger random action \n",
    "            action = agent.get_action(history)\n",
    "            # change action to real_action\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "\n",
    "            # A3.3 Take steps\n",
    "            observe, reward, done, truncation, info = env.step(real_action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            print(\"0: \", observe.shape)\n",
    "            print(\"1: \", next_state.shape)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            print(\"2: \", next_state.shape)\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            print(\"3: \", next_history.shape)\n",
    "            print(\"4: \", history.shape)\n",
    "            print(\"5: \", next_state.shape)\n",
    "            cv2.imshow(\"Frame\", observe)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "            # if the agent missed ball, agent is dead --> episode is not over\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.replay_memory(history, action, reward, next_history, dead)\n",
    "            # every some time interval, train model\n",
    "            agent.train_replay()\n",
    "            # update the target model with model\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            # if agent is dead, then reset the history\n",
    "            if dead:\n",
    "                dead = False\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            # if done, plot the score over episodes\n",
    "            if done:\n",
    "                if global_step > agent.train_start:\n",
    "                    stats = [score, agent.avg_q_max / float(step), step,\n",
    "                             agent.avg_loss / float(step)]\n",
    "                    for i in range(len(stats)):\n",
    "                        agent.sess.run(agent.update_ops[i], feed_dict={\n",
    "                            agent.summary_placeholders[i]: float(stats[i])\n",
    "                        })\n",
    "                    summary_str = agent.sess.run(agent.summary_op)\n",
    "                    agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon,\n",
    "                      \"  global_step:\", global_step, \"  average_q:\",\n",
    "                      agent.avg_q_max / float(step), \"  average loss:\",\n",
    "                      agent.avg_loss / float(step))\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/breakout_dqn.h5\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
